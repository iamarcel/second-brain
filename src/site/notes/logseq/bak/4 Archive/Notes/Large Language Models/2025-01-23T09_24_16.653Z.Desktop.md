---
{"dg-publish":true,"permalink":"/logseq/bak/4-archive/notes/large-language-models/2025-01-23-t09-24-16-653-z-desktop/"}
---


-
- [Artificial Analysis](https://artificialanalysis.ai/) has a great breakdown of LLM quality vs cost vs latency vs speed
- (mini-omni2)[https://github.com/gpt-omni/mini-omni2] is a project to make something like GPT-4o with multimodal and speech duplex capability
- ## Subtopics
- [[4 Archive/Notes/Prompt Engineering\|Prompt Engineering]]
- [[0 Inbox/ChatGPT\|ChatGPT]]
- [[LLM Agents\|LLM Agents]]
- ## Projects
- [[4 Archive/Notes/LifeGPT - The Bot For Life - Agentic Chatbot\|LifeGPT - The Bot For Life - Agentic Chatbot]]
- [[0 Inbox/AI Workshop\|AI Workshop]]
- [[4 Archive/Notes/AI Resume and Website Builder\|AI Resume and Website Builder]]
- ## Notes
- [[4 Archive/Notes/My Life Path - Proposed by ChatGPT\|My Life Path - Proposed by ChatGPT]]
- ## Tools
- [H2OGPT](https://github.com/h2oai/h2ogpt) Local web-based interface for chatting with document db indexes etc.
- [Lit-GPT](https://github.com/Lightning-AI/litgpt/tree/main) - Pretrain, finetune, deploy 20+ LLMs on your own data. Uses state-of-the-art techniques: flash attention, FSDP, 4-bit, LoRA, and more.
- [Instructor Embedding](https://github.com/xlang-ai/instructor-embedding) - Embedding specifically adapted to instructions
- # Finetuning
  Finetuning delivers better results than RAG, but requires a training step and data prepared in the right format.
  
  Tools
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)